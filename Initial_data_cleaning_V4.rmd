---
title: "Data Visualization and Story Telling: Group Project"
author: "Study Group 11: Alberto Lambert, Sijia Liu, Abhinav Bhardwaj, Bartek Makuch, Anna Plaschke, Feiyang Ni"
date: "2020-11-15"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---
```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=8, 
  fig.height=5,
  fig.align = "left"
)
```

```{r, load_libraries, include = FALSE}
library(readr) #load dataset
library(tidyverse)
library(lubridate)
library(janitor)
library(skimr)
library(sf)
library(ggmap)
library(ggrepel)
library(gridExtra)
library(pander)
library(here)
library(leaflet)
library(tmap)
library(tmaptools)
library(hrbrthemes)
library(patchwork)
library(kableExtra)
library(scales)
library(here)
library(janitor)
library(broom)
library(tidyquant)
library(infer)
library(openintro)
library(knitr)
library(patchwork)
library(scales)
library(tidytext)
library(plotly)
library(corrplot)
library(leaflet)
library(rsample) 
library(ggfortify)
library(car)
library(huxtable)
library(Hmisc) #describe function
library(here)
library(vroom)

```

# Load data

```{r loading_data, include = FALSE}
#loading first data set, incl. GDP
owid_covid_data <- read_csv(here::here("owid-covid-data.csv")) 
#cleaning the names
covid_data <- owid_covid_data %>% 
  clean_names()

#loading second data set: testing data
covid_testing_all_observations <- read_csv(here::here("covid-testing-all-observations.csv"))
#cleaning the names
covid_testing<- covid_testing_all_observations %>% 
  clean_names()

#loading third data set: number of hospitalized patients

hospital_data_original <- vroom(here::here("data.csv")) 
#cleaning the names
hospital_data <- hospital_data_original %>% 
  clean_names()
```

# Clean data
## dataset 1: owid-covid-data

We start with the `covid_data`, which has 48 covid-related indicators of a country in a given date. From the `skim` below, we observe:

* 17 completely empty columns 
* Incompleteness in almost all columns

```{r dataset1 skim}
skim(covid_data)
```

```{r dataset1 remove duplicates and empties}
#check for duplicates
covid_data %>% get_dupes(date, iso_code) # no duplicates :)
#Remove empty columns and rows
covid_data_clean <- remove_empty(covid_data, which = c("rows","cols")) # 17 columns and 0 rows removed
skim(covid_data_clean)
```
We start to clean each columns. We can see there are some rows with `location = International`which makes our `iso_code` NA. Since our analysis mainly focus on each countries, we will delete these rows. For the `continent` variable, we found that its NA come from`location = 'World'`, which is actually derivable from other rows, thus we will delete them.

```{r dataset1 clean iso_code and continent}
# NAs in iso_code
head(covid_data_clean[is.na(covid_data$iso_code),])
# delete them
covid_data_clean <- 
  covid_data_clean %>% filter(!is.na(iso_code))
# NAs in continent
head(covid_data_clean[is.na(covid_data_clean$continent),])
covid_data_clean <- covid_data_clean %>% 
  filter(!is.na(continent))
```

For the `gdp_per_capita` column, there are 28 countries/territories with missing values. And they are missing across all the days. We impute some of them, with data from world bank if it is availble, and leave the rest in NAs

```{r dataset1 clean gdp_per_capita}
# check if the gdp_per_capita of countries are NA all the time 
covid_data_clean %>% 
  filter(is.na(gdp_per_capita)) %>% 
  group_by(location) %>% 
  summarise(all_na = all(is.na(gdp_per_capita)))

covid_data_clean[covid_data_clean$location =='Andorra','gdp_per_capita'] = 45887
covid_data_clean[covid_data_clean$location =='Cuba','gdp_per_capita'] = 6816
covid_data_clean[covid_data_clean$location =='Faeroe Islands','gdp_per_capita'] = 48530
covid_data_clean[covid_data_clean$location =='Greenland','gdp_per_capita'] = 49311
covid_data_clean[covid_data_clean$location =='Guam','gdp_per_capita'] = 31477
covid_data_clean[covid_data_clean$location =='Isle of Man','gdp_per_capita'] = 90190
covid_data_clean[covid_data_clean$location =='Liechtenstein','gdp_per_capita'] = 141200
covid_data_clean[covid_data_clean$location =='Monaco','gdp_per_capita'] = 196061
covid_data_clean[covid_data_clean$location =='Northern Mariana Islands','gdp_per_capita'] = 18577
covid_data_clean[covid_data_clean$location =='Turks and Caicos Islands','gdp_per_capita'] = 21028
covid_data_clean[covid_data_clean$location =='United States Virgin Islands','gdp_per_capita'] = 11069
```
```{r}
skim(covid_data_clean)
```

How to deal with negative values, NAs in cases & deaths...? 

---------Abhinav----------

## Dataset2- covid_testing

```{r}

skim(covid_testing)
```


1) The data types are consistent with the type of data they represent.
2) All numerical variables have missing data
3) 





We notice that there are 118 unique values in entity while only 110 unique values in iso_code. let's see the distribution of these 

Distribution of entity
```{r}

covid_testing %>% tabyl(entity)%>%adorn_pct_formatting(digits = 2, affix_sign = TRUE)

```


Distribution of iso_code
```{r}
covid_testing %>% tabyl(iso_code)%>%adorn_pct_formatting(digits = 2, affix_sign = TRUE)
```

As seen from the result below, there are entries in "entity" with samples tested and people tested for the same country(having same iso_code). So all aggregations must be done on iso_code
```{r}

covid_testing[covid_testing$entity == "India - samples tested",]
covid_testing[covid_testing$entity == "India - people tested",]
```


Now let's look at the numerical variables

Distribution of cumulative_total
```{r}

max(covid_testing$cumulative_total,na.rm=TRUE)
min(covid_testing$cumulative_total,na.rm=TRUE)
ggplot(covid_testing, aes(x=cumulative_total)) + geom_boxplot()
ggplot(covid_testing, aes(x=log(cumulative_total))) +geom_histogram(binwidth=1)

```
We can see that there are no negative values. cumulative_total_per_thousand is derived from this variable and we don't analyze that seperately


Distribution of daily_change_in_cumulative_total
```{r}

max(covid_testing$daily_change_in_cumulative_total,na.rm=TRUE)
min(covid_testing$daily_change_in_cumulative_total,na.rm=TRUE)
ggplot(covid_testing, aes(x=daily_change_in_cumulative_total)) + geom_boxplot()
ggplot(covid_testing, aes(x=log(daily_change_in_cumulative_total))) +geom_histogram(binwidth=1)
```
We see that there are negative values which is fine as the change can be negative 

Distribution of x7_day_smoothed_daily_change
```{r}
max(covid_testing$x7_day_smoothed_daily_change,na.rm=TRUE)
min(covid_testing$x7_day_smoothed_daily_change,na.rm=TRUE)
ggplot(covid_testing, aes(x=x7_day_smoothed_daily_change)) + geom_boxplot()
ggplot(covid_testing, aes(x=log(x7_day_smoothed_daily_change))) +geom_histogram(binwidth=1)

```

short_term_tests_per_case distribution
```{r}
max(covid_testing$short_term_tests_per_case,na.rm=TRUE)
min(covid_testing$short_term_tests_per_case,na.rm=TRUE)
ggplot(covid_testing, aes(x=short_term_tests_per_case)) + geom_boxplot()
ggplot(covid_testing, aes(x=log(short_term_tests_per_case))) +geom_histogram(binwidth=1)


```


short_term_positive_rate Distribution 

```{r}
max(covid_testing$short_term_positive_rate,na.rm=TRUE)
min(covid_testing$short_term_positive_rate,na.rm=TRUE)
ggplot(covid_testing, aes(x=short_term_positive_rate)) + geom_boxplot()
ggplot(covid_testing, aes(x=short_term_positive_rate)) +geom_histogram(binwidth=0.05)


```


# Dealing with NA values

 

```{r}
# Subset of dataset which has all of these columns as NA
nrow(covid_testing[is.na(covid_testing$cumulative_total) & is.na(covid_testing$daily_change_in_cumulative_total)& is.na(covid_testing$x7_day_smoothed_daily_change) & is.na(covid_testing$short_term_tests_per_case) & is.na(covid_testing$short_term_positive_rate),])

# There are 296 such rows. We remove them 
covid_testing <- covid_testing[!(is.na(covid_testing$cumulative_total) & is.na(covid_testing$daily_change_in_cumulative_total)& is.na(covid_testing$x7_day_smoothed_daily_change) & is.na(covid_testing$short_term_tests_per_case) & is.na(covid_testing$short_term_positive_rate)),]

# Next, we remove all rows with no value for cumulative_total

covid_testing <- covid_testing %>%
  filter(!is.na(cumulative_total))

skim(covid_testing)
```

Looking at the new dataset, daily_change_in_cumulative_total is missing even where cumulative_total is present. So we will build this variable later from cumulative total.

Let's look at the other variables

``` {r}

covid_testing[c("iso_code","date","x7_day_smoothed_daily_change")] %>% 
  group_by(iso_code) %>% 
  arrange(date) %>%
  slice(1:9)

```
We see that the first few values are missing for each iso_code. We will impute this variable by 0

x7_day_smoothed_daily_change imputation by 0 and since x7_day_smoothed_daily_change_per_thousand is derrived from this variable, we will impute that by 0 as well
```{r}
covid_testing$x7_day_smoothed_daily_change[is.na(covid_testing$x7_day_smoothed_daily_change)] <-0
covid_testing$x7_day_smoothed_daily_change[is.na(covid_testing$x7_day_smoothed_daily_change_per_thousand)] <-0

```


Check dupes
```{r}



#check for duplicates dataset 2
dupes_testing<-covid_testing%>%get_dupes(date, iso_code)
covid_testing1 <- covid_testing %>% distinct(date, iso_code, .keep_all = TRUE)
#test if removing duplicates worked
dupes_testing2 <- covid_testing1%>%get_dupes(date, iso_code)

covid_testing2 <- covid_testing1 %>% 
  group_by(date, iso_code) %>% 
  summarise(cumulative_total_1 = sum(cumulative_total))

```



##  Dataset-3 - Hospital Data

```{r}

skim(hospital_data)
```



```{r}

hospital_data %>% tabyl(indicator)%>%adorn_pct_formatting(digits = 2, affix_sign = TRUE)

```


We remove all rows with no indicator as we don't know what it would stand for in cases where indicator = NA

```{r}
hospital_data <- hospital_data %>%
  filter(!is.na(indicator))
```


```{r}
skim(hospital_data)
```
Next we remove rows which are not populated in the filed - "values"

```{r}

hospital_data <- hospital_data %>%
  filter(!is.na(value))
  

```

We still have 621 rows with no date. However we have year_week populated for them

Pivot hospital data in order to clean it and then merge with covid_data_clean
```{r}

# hospital_data1 <- hospital_data %>% 
#   select(-c(source, url, year_week)) %>% #only daily data
#   pivot_wider( names_from=indicator, values_from=value) %>% 
#   select(c(1:4)) #deselect weekly indicators


hospital_data1 <- hospital_data %>% 
  pivot_wider( names_from=indicator, values_from=value) %>% 
  select(c(1, 2, 6, 7 )) %>% 
  clean_names()

  
 #deselect weekly indicators


dupes_hospital<-hospital_data1%>%get_dupes(date, country) #duplicates resulted from different sources

hospital_data2 <- hospital_data1 %>% 
  group_by(date, country) %>% 
  summarise(daily_hospital_occupancy = sum(daily_hospital_occupancy),
            daily_icu_occupancy = sum(daily_icu_occupancy))

dupes_hospital2<-hospital_data2%>%get_dupes(date, country) #no duplicates

```
## Merging dataset 1 with dataset3


```{r}

#rename columns in dataset 3 in order to join datasets

hospital_data2 <- hospital_data2 %>% 
  rename( location = country)

#merging datasets

covid_total <- covid_data_clean %>% 
  left_join(hospital_data2, by= c("location", "date"))

#see what's inside

skim(covid_total)

```

We observe that there are still quite a few missing values, however we are going to focus on Europe in our analysis.

```{r}
#filter for European countries

unique(covid_total$continent)

covid_europe <- covid_total %>% 
  filter(continent == "Europe") %>% 
  clean_names()

skim(covid_europe)




```



